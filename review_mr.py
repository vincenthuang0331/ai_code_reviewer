#!/usr/bin/env python3
"""GitLab MR reviewer powered by LLM"""

from config import (
    validate_config,
    get_provider_from_model,
    SERVER_URL,
    PROJECT_ID,
    MR_IID,
    AI_ACCESS_KEY,
    AI_MODEL,
    POST_COMMENT,
    MAX_BATCH_CHARS,
    MAX_BATCH_FILES,
    FILE_PATTERN,
)
from gitlab_client import get_mr_diff, post_comment
from llm import get_llm_client
from prompts import build_review_prompt
from formatter import format_review_output


def create_batches(files):
    """å°‡æª”æ¡ˆåˆ†çµ„ç‚ºæ‰¹æ¬¡"""
    batches = []
    current_batch = []
    current_batch_size = 0
    
    for file_info in files:
        diff_size = len(file_info['diff'])
        
        # å¦‚æœå–®å€‹æª”æ¡ˆè¶…éæ‰¹æ¬¡é™åˆ¶ï¼Œå–®ç¨è™•ç†
        if diff_size > MAX_BATCH_CHARS:
            if current_batch:
                batches.append(current_batch)
                current_batch = []
                current_batch_size = 0
            batches.append([file_info])
        # å¦‚æœåŠ å…¥ç•¶å‰æ‰¹æ¬¡æœƒè¶…éé™åˆ¶ï¼Œé–‹å§‹æ–°æ‰¹æ¬¡
        elif (current_batch_size + diff_size > MAX_BATCH_CHARS or 
              len(current_batch) >= MAX_BATCH_FILES):
            batches.append(current_batch)
            current_batch = [file_info]
            current_batch_size = diff_size
        # åŠ å…¥ç•¶å‰æ‰¹æ¬¡
        else:
            current_batch.append(file_info)
            current_batch_size += diff_size
    
    # åŠ å…¥æœ€å¾Œä¸€å€‹æ‰¹æ¬¡
    if current_batch:
        batches.append(current_batch)
    
    return batches


def process_batches(batches, mr_data, llm_client):
    """è™•ç†æ‰€æœ‰æ‰¹æ¬¡ä¸¦æ”¶é›†å•é¡Œ"""
    all_issues = []
    
    for batch_idx, batch in enumerate(batches, 1):
        file_paths = [f['file_path'] for f in batch]
        if len(batch) == 1:
            print(f"\n[æ‰¹æ¬¡ {batch_idx}/{len(batches)}] æ­£åœ¨å¯©æŸ¥: {file_paths[0]}")
        else:
            print(f"\n[æ‰¹æ¬¡ {batch_idx}/{len(batches)}] æ­£åœ¨å¯©æŸ¥ {len(batch)} å€‹æª”æ¡ˆ:")
            for fp in file_paths:
                print(f"  - {fp}")
        
        # æ§‹å»º promptï¼ˆèˆ‡ LLM ç„¡é—œï¼‰
        if len(batch) == 1:
            file_info = f"æª”æ¡ˆ: {batch[0]['file_path']}"
            diff_content = batch[0]['diff']
        else:
            file_info = f"æª”æ¡ˆæ•¸é‡: {len(batch)}"
            diff_parts = []
            for fd in batch:
                diff_parts.append(f"\n{'='*80}\næª”æ¡ˆ: {fd['file_path']}\n{'='*80}\n{fd['diff']}")
            diff_content = "\n".join(diff_parts)
        
        prompt = build_review_prompt(
            mr_data['title'],
            mr_data['description'],
            file_info,
            diff_content
        )
        
        # å‘¼å« LLM å¯©æŸ¥
        issues = llm_client.review_code(prompt)
        
        if issues:
            all_issues.extend(issues)
            print(f"âœ… å®Œæˆå¯©æŸ¥: ç™¼ç¾ {len(issues)} å€‹å•é¡Œ")
        else:
            print(f"âœ… å®Œæˆå¯©æŸ¥: ç„¡å•é¡Œ")
    
    return all_issues


def main():
    """ä¸»ç¨‹å¼æµç¨‹"""
    # é©—è­‰è¨­å®š
    validate_config()
    
    # åˆ¤æ–· LLM æä¾›å•†
    llm_provider = get_provider_from_model(AI_MODEL)
    
    # é¡¯ç¤ºè¨­å®šè³‡è¨Š
    print("=" * 80)
    print("GitLab MR Code Reviewer")
    print("=" * 80)
    print(f"GitLab URL: {SERVER_URL}")
    print(f"Project ID: {PROJECT_ID}")
    print(f"MR IID: {MR_IID}")
    print(f"LLM Provider: {llm_provider}")
    print(f"AI Model: {AI_MODEL}")
    print(f"Post Comment: {POST_COMMENT}")
    print("=" * 80)
    
    # å»ºç«‹ LLM å®¢æˆ¶ç«¯
    llm_client = get_llm_client(
        model=AI_MODEL,
        api_key=AI_ACCESS_KEY
    )

    # ç²å– MR diff
    mr_data = get_mr_diff()
    file_count = len(mr_data['files'])
    print(f"âœ… æˆåŠŸç²å– MR diff ({file_count} å€‹ç¬¦åˆæª”æ¡ˆ)")

    if file_count == 0:
        print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¨¡å¼çš„æª”æ¡ˆ (FILE_PATTERN={FILE_PATTERN})ï¼ŒçµæŸå¯©æŸ¥ã€‚")
        return

    # æ‰¹æ¬¡è™•ç†
    batches = create_batches(mr_data['files'])
    print(f"\nğŸ“¦ å·²å°‡ {file_count} å€‹æª”æ¡ˆåˆ†æˆ {len(batches)} å€‹æ‰¹æ¬¡è™•ç†")
    
    # è™•ç†æ‰€æœ‰æ‰¹æ¬¡
    all_issues = process_batches(batches, mr_data, llm_client)

    # æ ¼å¼åŒ–è¼¸å‡º
    project_path = mr_data.get('project_path', '')
    source_branch = mr_data['source_branch']
    combined_review = format_review_output(all_issues, project_path, source_branch)

    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 80)
    print("å¯©æŸ¥çµæœ")
    print("=" * 80)
    print(combined_review)
    print("=" * 80)

    # ç™¼ä½ˆè©•è«–
    post_comment(combined_review)
    print("\nâœ… å¯©æŸ¥å®Œæˆï¼")


if __name__ == "__main__":
    main()
